{"version":3,"file":"llms.d.ts","names":["CallbackManagerForLLMRun","BaseLLMParams","LLM","BaseLanguageModelCallOptions","GenerationChunk","LLMResult","FIMCompletionRequest","MistralAIFIMCompletionRequest","FIMCompletionStreamRequest","MistralAIFIMCompletionStreamRequest","FIMCompletionResponse","MistralAIFIMCompletionResponse","ChatCompletionResponse","MistralAIChatCompletionResponse","CompletionEvent","MistralAIChatCompletionEvent","BeforeRequestHook","RequestErrorHook","ResponseHook","HTTPClient","MistralAIHTTPClient","MistralAICallOptions","MistralAIInput","MistralAI","Array","Omit","Promise","AsyncIterable","AsyncGenerator"],"sources":["../src/llms.d.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseLLMParams, LLM } from \"@langchain/core/language_models/llms\";\nimport { type BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { GenerationChunk, LLMResult } from \"@langchain/core/outputs\";\nimport { FIMCompletionRequest as MistralAIFIMCompletionRequest } from \"@mistralai/mistralai/models/components/fimcompletionrequest.js\";\nimport { FIMCompletionStreamRequest as MistralAIFIMCompletionStreamRequest } from \"@mistralai/mistralai/models/components/fimcompletionstreamrequest.js\";\nimport { FIMCompletionResponse as MistralAIFIMCompletionResponse } from \"@mistralai/mistralai/models/components/fimcompletionresponse.js\";\nimport { ChatCompletionResponse as MistralAIChatCompletionResponse } from \"@mistralai/mistralai/models/components/chatcompletionresponse.js\";\nimport { CompletionEvent as MistralAIChatCompletionEvent } from \"@mistralai/mistralai/models/components/completionevent.js\";\nimport { BeforeRequestHook, RequestErrorHook, ResponseHook, HTTPClient as MistralAIHTTPClient } from \"@mistralai/mistralai/lib/http.js\";\nexport interface MistralAICallOptions extends BaseLanguageModelCallOptions {\n    /**\n     * Optional text/code that adds more context for the model.\n     * When given a prompt and a suffix the model will fill what\n     * is between them. When suffix is not provided, the model\n     * will simply execute completion starting with prompt.\n     */\n    suffix?: string;\n}\nexport interface MistralAIInput extends BaseLLMParams {\n    /**\n     * The name of the model to use.\n     * @default \"codestral-latest\"\n     */\n    model?: string;\n    /**\n     * The API key to use.\n     * @default {process.env.MISTRAL_API_KEY}\n     */\n    apiKey?: string;\n    /**\n     * Override the default server URL used by the Mistral SDK.\n     * @deprecated use serverURL instead\n     */\n    endpoint?: string;\n    /**\n     * Override the default server URL used by the Mistral SDK.\n     */\n    serverURL?: string;\n    /**\n     * What sampling temperature to use, between 0.0 and 2.0.\n     * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n     * @default {0.7}\n     */\n    temperature?: number;\n    /**\n     * Nucleus sampling, where the model considers the results of the tokens with `topP` probability mass.\n     * So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n     * Should be between 0 and 1.\n     * @default {1}\n     */\n    topP?: number;\n    /**\n     * The maximum number of tokens to generate in the completion.\n     * The token count of your prompt plus maxTokens cannot exceed the model's context length.\n     */\n    maxTokens?: number;\n    /**\n     * Whether or not to stream the response.\n     * @default {false}\n     */\n    streaming?: boolean;\n    /**\n     * The seed to use for random sampling. If set, different calls will generate deterministic results.\n     * Alias for `seed`\n     */\n    randomSeed?: number;\n    /**\n     * Batch size to use when passing multiple documents to generate\n     */\n    batchSize?: number;\n    /**\n     * A list of custom hooks that must follow (req: Request) => Awaitable<Request | void>\n     * They are automatically added when a ChatMistralAI instance is created\n     */\n    beforeRequestHooks?: BeforeRequestHook[];\n    /**\n     * A list of custom hooks that must follow (err: unknown, req: Request) => Awaitable<void>\n     * They are automatically added when a ChatMistralAI instance is created\n     */\n    requestErrorHooks?: RequestErrorHook[];\n    /**\n     * A list of custom hooks that must follow (res: Response, req: Request) => Awaitable<void>\n     * They are automatically added when a ChatMistralAI instance is created\n     */\n    responseHooks?: ResponseHook[];\n    /**\n     * Optional custom HTTP client to manage API requests\n     * Allows users to add custom fetch implementations, hooks, as well as error and response processing.\n     */\n    httpClient?: MistralAIHTTPClient;\n    /**\n     * Whether to use the Fill-In-Middle (FIM) API for code completion.\n     * When true, uses `client.fim.complete()` / `client.fim.stream()`.\n     * When false, uses `client.chat.complete()` / `client.chat.stream()` with the prompt wrapped as a user message.\n     *\n     * FIM is only supported for code models like `codestral-latest`.\n     * For general-purpose models like `mistral-large-latest`, set this to `false`.\n     *\n     * @default true for codestral models, false for other models\n     */\n    useFim?: boolean;\n}\n/**\n * MistralAI completions LLM.\n */\nexport declare class MistralAI extends LLM<MistralAICallOptions> implements MistralAIInput {\n    static lc_name(): string;\n    lc_namespace: string[];\n    lc_serializable: boolean;\n    model: string;\n    temperature: number;\n    topP?: number;\n    maxTokens?: number | undefined;\n    randomSeed?: number | undefined;\n    streaming: boolean;\n    batchSize: number;\n    apiKey: string;\n    /**\n     * @deprecated use serverURL instead\n     */\n    endpoint: string;\n    serverURL?: string;\n    maxRetries?: number;\n    maxConcurrency?: number;\n    beforeRequestHooks?: Array<BeforeRequestHook>;\n    requestErrorHooks?: Array<RequestErrorHook>;\n    responseHooks?: Array<ResponseHook>;\n    httpClient?: MistralAIHTTPClient;\n    useFim: boolean;\n    constructor(fields?: MistralAIInput);\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_aliases(): {\n        [key: string]: string;\n    } | undefined;\n    _llmType(): string;\n    invocationParams(options: this[\"ParsedCallOptions\"]): Omit<MistralAIFIMCompletionRequest | MistralAIFIMCompletionStreamRequest, \"prompt\">;\n    /**\n     * For some given input string and options, return a string output.\n     *\n     * Despite the fact that `invoke` is overridden below, we still need this\n     * in order to handle public APi calls to `generate()`.\n     */\n    _call(prompt: string, options: this[\"ParsedCallOptions\"]): Promise<string>;\n    _generate(prompts: string[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<LLMResult>;\n    completionWithRetry(request: MistralAIFIMCompletionRequest, options: this[\"ParsedCallOptions\"], stream: false): Promise<MistralAIFIMCompletionResponse | MistralAIChatCompletionResponse>;\n    completionWithRetry(request: MistralAIFIMCompletionStreamRequest, options: this[\"ParsedCallOptions\"], stream: true): Promise<AsyncIterable<MistralAIChatCompletionEvent>>;\n    _streamResponseChunks(prompt: string, options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n    addAllHooksToHttpClient(): void;\n    removeAllHooksFromHttpClient(): void;\n    removeHookFromHttpClient(hook: BeforeRequestHook | RequestErrorHook | ResponseHook): void;\n    private imports;\n}\n//# sourceMappingURL=llms.d.ts.map"],"mappings":";;;;;;;;;;;;UAUiBqB,oBAAAA,SAA6BlB;;AAA9C;AASA;;;;EAuEiBiB,MAAAA,CAAAA,EAAAA,MAAAA;;AAvEoC,UAApCE,cAAAA,SAAuBrB,aAAa,CAAA;EAuFhCsB;;;;EAoBSN,KAAAA,CAAAA,EAAAA,MAAAA;EAANO;;;;EAICF,MAAAA,CAAAA,EAAAA,MAAAA;EAQsCf;;;;EAQmBP,QAAAA,CAAAA,EAAAA,MAAAA;EAAmCK;;;EACOM,SAAAA,CAAAA,EAAAA,MAAAA;EAAiCE;;;;;EACpCa,WAAAA,CAAAA,EAAAA,MAAAA;EAC9B1B;;;;;;EA3CpDE,IAAAA,CAAAA,EAAAA,MAAAA;EAAqCoB;AAAc;;;;;;;;;;;;;;;;;;;;;;uBA/BjEN;;;;;sBAKDC;;;;;kBAKJC;;;;;eAKHE;;;;;;;;;;;;;;;;cAgBIG,SAAAA,SAAkBrB,IAAImB,iCAAiCC;;;;;;;;;;;;;;;;;;;uBAmBnDE,MAAMR;sBACPQ,MAAMP;kBACVO,MAAMN;eACTE;;uBAEQE;;;;;;;;wDAQiCG,KAAKlB,uBAAgCE;;;;;;;6DAOhCiB;gFACmB1B,2BAA2B0B,QAAQrB;+BACpFE,0EAAmFmB,QAAQf,wBAAiCE;+BAC5HJ,+EAAwFiB,QAAQC,cAAcZ;yFACpDf,2BAA2B4B,eAAexB;;;iCAGlGY,oBAAoBC,mBAAmBC"}